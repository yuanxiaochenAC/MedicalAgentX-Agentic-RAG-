"""
åŸºäºEvoAgentXçš„åŒ»ç–—RAGå¼•æ“
EvoAgentX-based Medical RAG Engine
"""

import os
import json
import logging
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

from evoagentx.core.logging import logger
from evoagentx.storages.base import StorageHandler
from evoagentx.rag.rag import RAGEngine
from evoagentx.rag.schema import Query, Corpus, Chunk, ChunkMetadata

from evoagentx_medical_config import (
    DATA_DIR, CACHE_DIR, STORAGE_CONFIG, RAG_CONFIG,
    PROJECT_ROOT, OUTPUTS_DIR
)

class MedicalRAGEngine:
    """åŸºäºEvoAgentXçš„åŒ»ç–—æ–‡æ¡£RAGå¼•æ“"""
    
    def __init__(self):
        """åˆå§‹åŒ–åŒ»ç–—RAGå¼•æ“"""
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–å­˜å‚¨å¤„ç†å™¨
        self.storage_handler = StorageHandler(storageConfig=STORAGE_CONFIG)
        
        # åˆå§‹åŒ–RAGå¼•æ“
        self.rag_engine = RAGEngine(
            config=RAG_CONFIG,
            storage_handler=self.storage_handler
        )
        
        self.corpus_name = "medical_cases"
        self.is_indexed = False
        
        self.logger.info("åŒ»ç–—RAGå¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def index_medical_documents(self, force_reindex: bool = False) -> bool:
        """ç´¢å¼•åŒ»å­¦PDFæ–‡æ¡£"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç»å»ºç«‹ç´¢å¼•
            if not force_reindex and self._check_existing_index():
                self.logger.info("å‘ç°ç°æœ‰ç´¢å¼•ï¼Œè·³è¿‡é‡å»º")
                self.is_indexed = True
                return True
            
            # è·å–PDFæ–‡ä»¶åˆ—è¡¨
            pdf_files = list(DATA_DIR.glob("*.pdf"))
            if not pdf_files:
                self.logger.warning(f"åœ¨ {DATA_DIR} ä¸­æœªæ‰¾åˆ°PDFæ–‡ä»¶")
                return False
            
            self.logger.info(f"å¼€å§‹ç´¢å¼• {len(pdf_files)} ä¸ªåŒ»å­¦PDFæ–‡æ¡£...")
            
            # åˆ›å»ºè¯­æ–™åº“
            chunks = []
            
            # æ·»åŠ æ–‡æ¡£åˆ°è¯­æ–™åº“
            for pdf_file in pdf_files:
                self.logger.info(f"å¤„ç†æ–‡æ¡£: {pdf_file.name}")
                
                # ä½¿ç”¨EvoAgentXçš„æ–‡æ¡£å¤„ç†åŠŸèƒ½
                file_chunks = self._process_pdf_file(pdf_file)
                chunks.extend(file_chunks)
            
            # åˆ›å»ºå®Œæ•´çš„è¯­æ–™åº“
            corpus = Corpus(chunks=chunks, corpus_id=self.corpus_name)
            
            # å°†è¯­æ–™åº“æ·»åŠ åˆ°RAGå¼•æ“å¹¶å»ºç«‹ç´¢å¼•
            self.rag_engine.add(index_type="vector", nodes=corpus, corpus_id=self.corpus_name)
            
            self.is_indexed = True
            self.logger.info(f"æˆåŠŸç´¢å¼• {len(chunks)} ä¸ªæ–‡æ¡£å—")
            
            return True
            
        except Exception as e:
            self.logger.error(f"ç´¢å¼•æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return False
    
    def _process_pdf_file(self, pdf_file: Path) -> List[Chunk]:
        """å¤„ç†å•ä¸ªPDFæ–‡ä»¶"""
        import PyPDF2
        
        chunks = []
        try:
            # æå–PDFæ–‡æœ¬
            with open(pdf_file, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text += page.extract_text() + "\n"
            
            if not text.strip():
                self.logger.warning(f"æ— æ³•ä» {pdf_file.name} æå–æ–‡æœ¬")
                return chunks
            
            # ä½¿ç”¨RAGé…ç½®çš„åˆ†å—ç­–ç•¥
            chunk_size = RAG_CONFIG.chunker.chunk_size
            chunk_overlap = RAG_CONFIG.chunker.chunk_overlap
            
            # ç®€å•åˆ†å—ï¼ˆå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„åˆ†å—ç­–ç•¥ï¼‰
            text_chunks = self._chunk_text(text, chunk_size, chunk_overlap)
            
            # åˆ›å»ºChunkå¯¹è±¡
            for i, chunk_text in enumerate(text_chunks):
                if chunk_text.strip():
                    chunk = Chunk(
                        chunk_id=f"{pdf_file.stem}_{i}",
                        text=chunk_text,
                        metadata=ChunkMetadata(
                            doc_id=pdf_file.stem,
                            corpus_id=self.corpus_name,
                            chunk_index=i,
                            file_name=pdf_file.name,
                            file_path=str(pdf_file)
                        ),
                        start_char_idx=0,
                        end_char_idx=len(chunk_text),
                        excluded_embed_metadata_keys=[],
                        excluded_llm_metadata_keys=[],
                        relationships={}
                    )
                    chunks.append(chunk)
            
        except Exception as e:
            self.logger.error(f"å¤„ç†PDFæ–‡ä»¶ {pdf_file} æ—¶å‡ºé”™: {str(e)}")
        
        return chunks
    
    def _chunk_text(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """æ–‡æœ¬åˆ†å—"""
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            if end >= len(text):
                chunks.append(text[start:])
                break
            
            # å°è¯•åœ¨å¥å­è¾¹ç•Œåˆ†å—
            chunk = text[start:end]
            last_sentence_end = max(
                chunk.rfind('ã€‚'), 
                chunk.rfind('!'), 
                chunk.rfind('ï¼Ÿ'),
                chunk.rfind('.'),
                chunk.rfind('!'),
                chunk.rfind('?')
            )
            
            if last_sentence_end > chunk_size // 2:
                end = start + last_sentence_end + 1
            
            chunks.append(text[start:end])
            start = end - overlap
        
        return chunks
    
    def _check_existing_index(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç°æœ‰ç´¢å¼•"""
        try:
            # æ£€æŸ¥å­˜å‚¨ç›®å½•æ˜¯å¦å­˜åœ¨ç›¸å…³æ–‡ä»¶
            index_path = Path(STORAGE_CONFIG.path)
            if index_path.exists() and any(index_path.iterdir()):
                return True
            
            # å¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„ç´¢å¼•å­˜åœ¨æ€§æ£€æŸ¥
            return False
            
        except Exception:
            return False
    
    def search_similar_cases(self, query_text: str, top_k: int = 5) -> Dict[str, Any]:
        """æœç´¢ç›¸ä¼¼åŒ»å­¦ç—…ä¾‹"""
        try:
            if not self.is_indexed:
                self.logger.error("RAGå¼•æ“å°šæœªå»ºç«‹ç´¢å¼•")
                return {
                    "status": "error",
                    "error": "RAGå¼•æ“å°šæœªå»ºç«‹ç´¢å¼•",
                    "results": []
                }
            
            # åˆ›å»ºæŸ¥è¯¢å¯¹è±¡
            query = Query(query_str=query_text, top_k=top_k)
            
            # æ‰§è¡Œæ£€ç´¢
            rag_results = self.rag_engine.query(query, corpus_id=self.corpus_name)
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            if rag_results.corpus and rag_results.corpus.chunks:
                for i, chunk in enumerate(rag_results.corpus.chunks[:top_k]):
                    formatted_result = {
                        "rank": i + 1,
                        "content": chunk.text,
                        "score": getattr(chunk, 'score', 1.0),  # å¦‚æœæ²¡æœ‰åˆ†æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        "source": chunk.metadata.file_path,
                        "document_title": chunk.metadata.file_name,
                        "chunk_id": chunk.chunk_id,
                        "metadata": {
                            "chunk_index": chunk.metadata.chunk_index,
                            "doc_id": chunk.metadata.doc_id
                        }
                    }
                    formatted_results.append(formatted_result)
            
            return {
                "status": "success",
                "query": query_text,
                "total_results": len(formatted_results),
                "results": formatted_results,
                "search_metadata": {
                    "corpus_name": self.corpus_name,
                    "top_k": top_k,
                    "timestamp": datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            self.logger.error(f"æ£€ç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            return {
                "status": "error",
                "error": str(e),
                "results": []
            }
    
    def get_corpus_info(self) -> Dict[str, Any]:
        """è·å–è¯­æ–™åº“ä¿¡æ¯"""
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ è·å–è¯­æ–™åº“ç»Ÿè®¡ä¿¡æ¯çš„é€»è¾‘
            return {
                "corpus_name": self.corpus_name,
                "is_indexed": self.is_indexed,
                "storage_path": str(STORAGE_CONFIG.path),
                "total_documents": len(list(DATA_DIR.glob("*.pdf"))),
                "rag_config": {
                    "chunk_size": RAG_CONFIG.chunker.chunk_size,
                    "chunk_overlap": RAG_CONFIG.chunker.chunk_overlap,
                    "embedding_model": RAG_CONFIG.embedding.model_name,
                    "top_k": RAG_CONFIG.retrieval.top_k
                }
            }
        except Exception as e:
            self.logger.error(f"è·å–è¯­æ–™åº“ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
            return {}
    
    def save_search_results(self, results: Dict[str, Any], query: str) -> str:
        """ä¿å­˜æœç´¢ç»“æœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"medical_search_{timestamp}.json"
            filepath = OUTPUTS_DIR / "results" / filename
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            filepath.parent.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ç»“æœ
            save_data = {
                "query": query,
                "timestamp": datetime.now().isoformat(),
                "engine_info": self.get_corpus_info(),
                "search_results": results
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
            return str(filepath)
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}")
            return ""

def main():
    """æµ‹è¯•åŒ»ç–—RAGå¼•æ“"""
    print("ğŸ¥ åˆå§‹åŒ–åŒ»ç–—RAGå¼•æ“...")
    
    # åˆ›å»ºå¼•æ“
    medical_rag = MedicalRAGEngine()
    
    # å»ºç«‹ç´¢å¼•
    print("ğŸ“š æ­£åœ¨ç´¢å¼•åŒ»å­¦æ–‡æ¡£...")
    success = medical_rag.index_medical_documents()
    
    if not success:
        print("âŒ æ–‡æ¡£ç´¢å¼•å¤±è´¥")
        return
    
    print("âœ… æ–‡æ¡£ç´¢å¼•å®Œæˆ")
    
    # æµ‹è¯•æœç´¢
    test_queries = [
        "æ‚£è€…å‡ºç°å¤´ç—›ç—‡çŠ¶",
        "è¡€å‹å‡é«˜ä¼´è§†ç‰©æ¨¡ç³Š",
        "èƒ¸ç—›å‘¼å¸å›°éš¾å¿ƒç”µå›¾å¼‚å¸¸"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” æœç´¢: {query}")
        results = medical_rag.search_similar_cases(query, top_k=3)
        
        if results["status"] == "success":
            print(f"æ‰¾åˆ° {results['total_results']} ä¸ªç›¸ä¼¼ç—…ä¾‹:")
            for result in results["results"]:
                print(f"  - æ’å {result['rank']}: {result['document_title']}")
                print(f"    ç›¸ä¼¼åº¦: {result['score']:.4f}")
                print(f"    å†…å®¹: {result['content'][:100]}...")
                print()
        else:
            print(f"âŒ æœç´¢å¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    # æ˜¾ç¤ºè¯­æ–™åº“ä¿¡æ¯
    corpus_info = medical_rag.get_corpus_info()
    print("\nğŸ“Š è¯­æ–™åº“ä¿¡æ¯:")
    print(json.dumps(corpus_info, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    main()